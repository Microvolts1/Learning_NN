{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fully_connected_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO75zK5b25ulfF2FkbxRTU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Microvolts1/Learning_NN/blob/main/Fully_connected_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phân tích chi tiết cho Fully connected Layer."
      ],
      "metadata": {
        "id": "bbL1slD9zP2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer by layer\n",
        "- Ta thiết kế luồng hoạt động như sau:\n",
        "\n",
        "\n",
        "1.   Đầu tiên cho dữ liệu đầu vào NN.\n",
        "2.   Dữ liệu tiếp tục đi qua các layer khác nhau cho tới layer output.\n",
        "3.   Khi đã có output, ta có thể tính độ lỗi dùng làm độ đo.\n",
        "4.   Cuối cùng là điều chỉnh các tham số(weight - trọng số hoặc bias) bằng cách đạo hàm qua từng lớp tham số.\n",
        "5.   Lặp lại quá trình trên\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AVokv5U9GyYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mỗi layer gồm có:\n",
        "\n",
        "- Mỗi layer bất kì đều có điểm chung là input data và output data.\n",
        "<img src='https://miro.medium.com/max/1252/0*3cUc4jsQlHsoovn9.png'>\n",
        "\n"
      ],
      "metadata": {
        "id": "OSY219IkLHnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation - Lan truyền tiến.\n",
        "\n",
        "- Nghĩa là output của layer trước sẽ là input của lớp sau đó.\n",
        "<img src='https://miro.medium.com/max/1352/1*ddDKxWSAYci2dHaG3O_DOg.png'>\n",
        "\n",
        "- Khi đó, tại layer cuối ta sẽ cho ra output, ta gọi là $\\hat{Y}$, có giá trị sẽ lệch so với giá trị $Y$ thật. Lúc này ta sẽ tính toán độ lỗi $E$ thông qua hai giá trị trước đó, và mục đích tính toán độ lỗi nhằm để điều chỉnh các tham số nhằm giảm thiểu độ lỗi nhỏ nhất có thể. Và các điều chỉnh đó gọi là Backward propagation - Lan truyền ngược. \n",
        "\n"
      ],
      "metadata": {
        "id": "hUrP5mRaLiHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent.\n",
        "\n",
        "- Liên quan tới việc sử dụng đạo hàm, cụ thể hơn là ta sẽ điều chỉnh tham số $w$ trong NN nhằm mục đích giảm thiểu độ lỗi $E$ và đạo hàm là một trong những phương pháp hay để giải quyết.\n",
        "\n",
        "<img src='https://miro.medium.com/max/620/0*MZYPbIwBn5SOEpG7.png'   align=\"center\">\n",
        "\n",
        "- $\\alpha$ là gọi là learning rate với miền giá trị trong khoảng $[0,1]$"
      ],
      "metadata": {
        "id": "BdU_O3zUORyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Propagation - Lan truyền ngược.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "64H4dxXvQVFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Giả sử ta đã có output $Y$, ta sẽ thực hiện đạo hàm trên độ lỗi $E$ với output $Y$ và thực hiện ngược lại cho tới lớp input $X$.\n",
        "<img src='https://miro.medium.com/max/1400/0*bL0jVGoQiH_TsrvT.png'>\n",
        "\n",
        "- Lưu ý rằng $E$ là một con số còn $X$ và $Y$ là ma trận.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1052/0*KYFYEbu_o9j1_a5w.png'>\n",
        "\n",
        "\n",
        "- Và để tính được $\\frac{\\partial{E}}{\\partial{w}}$ thì áp dụng công thức toán học chain rule trong đạo hàm để tính toán:\n",
        "\n",
        "<img src='https://miro.medium.com/max/650/0*3xceKA-7b2oNbE4T.png'>"
      ],
      "metadata": {
        "id": "wExDonC-fPaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tiếp đến ta tính $\\frac{\\partial{E}}{\\partial{X}}$, lý do cần tính là vì output của layer này sẽ là input của layer khác nên ta cũng phải tính toán để có thể lan truyền đi tốt hơn.\n",
        "- Cũng sử dụng chain rule để ta có thể tính toán dễ dàng: \n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/658/0*Of8qDiWo31MK-MU0.png'>\n",
        "\n"
      ],
      "metadata": {
        "id": "kubbid2mfQ_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sơ đồ mô phong cách thức hoạt động của lan truyền ngược:\n",
        "1. Ban đầu ta có: \n",
        "\n",
        "<img src='https://miro.medium.com/max/1352/1*ddDKxWSAYci2dHaG3O_DOg.png'>\n",
        "\n",
        "\n",
        "2. Sau đó thực hiện lan truyền ngược:\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*0QPRST83oBicKPE_R4biJA.png'>"
      ],
      "metadata": {
        "id": "_Hm3SUTpf6Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thiết lập chi tiết cho các layer"
      ],
      "metadata": {
        "id": "XgWkDnrPgeb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base class Layer:\n",
        "- Đây sẽ là Base layer mà các Layer sẽ kế thừa. Chứa các thuộc tính đơn giản là input từ layer khác và output truyền đi. Kèm với phương thức foward và backward propagation."
      ],
      "metadata": {
        "id": "fa_4igHNgnc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Igaj160UyjN8"
      },
      "outputs": [],
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Connected Layer."
      ],
      "metadata": {
        "id": "qTNDT1Zrh9cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Đây là layer cơ bản nhất: Mỗi input nơ-ron sẽ nối hết tới tất cả output nơ-ron ở layer kế tiếp nó.\n",
        "\n",
        "<img src='https://miro.medium.com/max/876/1*jOObSF4HAB5VL5wO6petqA.png'>"
      ],
      "metadata": {
        "id": "5ZitjDzeiElW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation."
      ],
      "metadata": {
        "id": "YVuhStL2oQW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Việc lan tuyền tiến(Forward propagation) thực hiện như sau:\n",
        "\n",
        "<img src='https://miro.medium.com/max/674/0*Ec9fiXkNOoA5Z2v5.png'>\n",
        "\n",
        "- Với việc chúng ta sử dụng mà trận thì chúng ta có tích vô hướng (dot product) như sau:\n",
        "\n",
        "1. Ta có các ma trận:\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/0*qTBPwmzIjBOGKpXH.png'>\n",
        "\n",
        "\n",
        "2. Từ đó ta có được:\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/506/0*FsJQ82GmKlV2X22z.png'>"
      ],
      "metadata": {
        "id": "tQNAmbo4iEp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward propagation.\n"
      ],
      "metadata": {
        "id": "Lv8H9_AMoXM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Giả sử chúng ta đã có $Y$ và $E$, từ đó ta tính toán được $\\frac{\\partial{E}}{\\partial{Y}}$ hay đạo hàm độ lỗi với output. Tiếp đến ta cần tính toán 2 thứ:\n",
        "1. Đạo hàm độ lỗi $E$ với từng tham số: $\\frac{\\partial{E}}{\\partial{W}}$, $\\frac{\\partial{E}}{\\partial{B}}$\n",
        "\n",
        "2. Đạo hàm độ lỗi $E$ với input $X$: $\\frac{\\partial{E}}{\\partial{X}}$ "
      ],
      "metadata": {
        "id": "mLZ1Dq1BowUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Đầu tiên là tính $\\frac{\\partial{E}}{\\partial{W}}$, ma trận này nên cùng kích thước với ma trận $W$ với kích thước $i \\times j$. $i$ là số lượng input nơ-ron và $j$ là số lượng output nơ-ron.\n",
        "\n",
        "<img src='https://miro.medium.com/max/802/0*PiVZ-czmfvBaAFSe.png'>"
      ],
      "metadata": {
        "id": "0jr--u2rowaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sử dụng chain rule, ta có:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1242/0*cKYlxf87ZwkKtnrt.png'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4-DTsMrEsG4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cuối cùng ta rút ra được: \n",
        "\n",
        "<img src='https://miro.medium.com/max/1042/0*sEeQVqIapym6O9VH.png'>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zwDtAKvUsULm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tiếp đến ta tính $\\frac{\\partial{E}}{\\partial{B}}$\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1032/0*sJVXt05jf2cgd_Ys.png'>\n",
        "\n"
      ],
      "metadata": {
        "id": "xHd0W-x8yAm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ở đây ta tiếp tục sử dụng chain rule:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1114/0*ud43sxuKpgPo9Rlo.png'>\n",
        "\n",
        "\n",
        "- Cuối cùng rút ra được là: \n",
        "\n",
        "<img src='https://miro.medium.com/max/1046/0*sovSd27ja1_7R2yU.png'>"
      ],
      "metadata": {
        "id": "-3Q6RCVqzUm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tính toán cho $\\frac{\\partial{E}}{\\partial{X}}$\n",
        "\n",
        "<img src='https://miro.medium.com/max/1052/0*nPzn6Jgv-P0wxUA7.png'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j4nz1CV5zUpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Đầu tiên là chain rule:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1118/0*x6uE01GkG3NKLNQp.png'>\n",
        "\n",
        "\n",
        "- Cuối cùng rút ra được là: \n",
        "\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/0*JWCzIdtJVTeQ_PG8.png'>"
      ],
      "metadata": {
        "id": "0WuLsOsy0d9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tổng kết và coding layer."
      ],
      "metadata": {
        "id": "_Sppow300P9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ta có được các công thức để áp dụng như sau:\n",
        "\n",
        "<img src='https://miro.medium.com/max/492/0*HlI8qj8qZqGIWBrk.png'>"
      ],
      "metadata": {
        "id": "TjXPwzik05iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Và giờ ta code fully connected layer như dưới"
      ],
      "metadata": {
        "id": "crWdT4ul1OCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "C6evdmmENLna"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Layer - Lớp kích hoạt.\n",
        "\n",
        "- Giả sử ta có hai hàm kích hoạt $f$ và $f'$ tương ứng cho việc forward và backward.\n",
        "\n",
        "<img src='https://miro.medium.com/max/770/1*xl7UacJfCUAk_KqX3WI49Q.png'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yrhfd1d62HaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation.\n",
        "\n",
        "- Là hàm kích hoạt cho từng phần tử trong ma trận input.\n",
        "\n",
        "<img src='https://miro.medium.com/max/906/0*Aw9jCvpliMjaO00W.png'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Wz4h-HKd2Hju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward propagation.\n",
        "\n",
        "- Giả sử đã có $\\frac{\\partial{E}}{\\partial{Y}}$, giờ thì ta tính $\\frac{\\partial{E}}{\\partial{X}}$ cho layer này.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/0*9aXZPVNHXXpz1vd9.png'>\n",
        "\n",
        "\n",
        "- Lưu ý dòng cuối biểu tượng $\\odot$ ám chỉ phép toán [Hamada product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) nghĩa là ma trận cùng kích thước nhân tương ứng các phần tử với nhau. (VD: ma trận $A$,$B$ thì nhân tương ứng là $a_{ij} \\times b_{ij}$)"
      ],
      "metadata": {
        "id": "oHJvxsEa4uke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding layer"
      ],
      "metadata": {
        "id": "noW2xylZ6E-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error"
      ],
      "metadata": {
        "id": "cCrMyi5O0POr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function and its derivative\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-np.tanh(x)**2;"
      ],
      "metadata": {
        "id": "vOFRD6B1IEgL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function - Hàm mất mát."
      ],
      "metadata": {
        "id": "lUf1AesR6ZBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tính toán layer cuối cùng nhất."
      ],
      "metadata": {
        "id": "iPKlphqx6kfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Với các layer trước thì luôn có $\\frac{\\partial{E}}{\\partial{Y}}$ tính được từ layer sau. Nhưng với layer out cuối cùng nhất thì lấy ở đâu?\n",
        "\n",
        "- Đáp án chính là việc ta tính toán từ hàm mất mát. Một trong số đó là hàm MSE - Mean Square Error. \n",
        "\n",
        "<img src='https://miro.medium.com/max/726/0*CuLKvZnTvjT1d6KJ.png'>"
      ],
      "metadata": {
        "id": "93kW5pGR6piN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Đạo hàm ta có:\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1176/0*2ggVYLPNH-vIJir6.png'>"
      ],
      "metadata": {
        "id": "3ZpITkny7g-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding"
      ],
      "metadata": {
        "id": "UcjHzCc08O6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function and its derivative\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "ZRiXhv7q8On4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xây dựng Network."
      ],
      "metadata": {
        "id": "DZZu-8If8VpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            if (i % 100 == 0 or epochs < 101):\n",
        "              print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ],
      "metadata": {
        "id": "uTOvvG5O8Hl2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thử nghiệm."
      ],
      "metadata": {
        "id": "-29BpdIg9RWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Giải bài toán XOR."
      ],
      "metadata": {
        "id": "hr4vPQRf9VoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "x_train = np.array([[[0,1]], [[1,1]], [[1,0]], [[1,1]], [[0,0]]])\n",
        "y_train = np.array([[[0]], [[1]], [[0]], [[1]], [[0]]])\n",
        "\n",
        "# network\n",
        "net = Network()\n",
        "net.add(FCLayer(2, 3))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(3, 1))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# train\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "# test\n",
        "out = net.predict(x_train)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5RReHaH9a9O",
        "outputId": "3a6fe114-7248-43c7-dbfd-102cd8cad575"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/1000   error=0.327503\n",
            "epoch 101/1000   error=0.042206\n",
            "epoch 201/1000   error=0.009154\n",
            "epoch 301/1000   error=0.001532\n",
            "epoch 401/1000   error=0.000666\n",
            "epoch 501/1000   error=0.000416\n",
            "epoch 601/1000   error=0.000301\n",
            "epoch 701/1000   error=0.000235\n",
            "epoch 801/1000   error=0.000192\n",
            "epoch 901/1000   error=0.000162\n",
            "[array([[0.00120986]]), array([[0.98138596]]), array([[0.00147417]]), array([[0.98138596]]), array([[-0.00088333]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Giải bài toán MNIST."
      ],
      "metadata": {
        "id": "ShBD4lEm5us_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Network\n",
        "net = Network()\n",
        "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# train on 1000 samples\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
        "\n",
        "# test on 3 samples\n",
        "out = net.predict(x_test[1000:2000])\n",
        "#print(\"\\n\")\n",
        "#print(\"predicted values : \")\n",
        "#print(out, end=\"\\n\")\n",
        "#print(\"true values : \")\n",
        "#print(y_test[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxeFwxUx4Acn",
        "outputId": "cf05d55f-e440-44cd-f13f-7ccd8cecb4e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "epoch 0/35   error=0.226004\n",
            "epoch 1/35   error=0.090320\n",
            "epoch 2/35   error=0.073019\n",
            "epoch 3/35   error=0.062484\n",
            "epoch 4/35   error=0.054875\n",
            "epoch 5/35   error=0.049250\n",
            "epoch 6/35   error=0.044715\n",
            "epoch 7/35   error=0.040702\n",
            "epoch 8/35   error=0.037110\n",
            "epoch 9/35   error=0.033852\n",
            "epoch 10/35   error=0.030767\n",
            "epoch 11/35   error=0.027991\n",
            "epoch 12/35   error=0.025645\n",
            "epoch 13/35   error=0.023661\n",
            "epoch 14/35   error=0.021943\n",
            "epoch 15/35   error=0.020377\n",
            "epoch 16/35   error=0.019048\n",
            "epoch 17/35   error=0.017584\n",
            "epoch 18/35   error=0.016446\n",
            "epoch 19/35   error=0.015414\n",
            "epoch 20/35   error=0.014429\n",
            "epoch 21/35   error=0.013654\n",
            "epoch 22/35   error=0.012661\n",
            "epoch 23/35   error=0.011922\n",
            "epoch 24/35   error=0.011245\n",
            "epoch 25/35   error=0.010684\n",
            "epoch 26/35   error=0.010032\n",
            "epoch 27/35   error=0.009553\n",
            "epoch 28/35   error=0.009035\n",
            "epoch 29/35   error=0.008585\n",
            "epoch 30/35   error=0.008150\n",
            "epoch 31/35   error=0.007781\n",
            "epoch 32/35   error=0.007494\n",
            "epoch 33/35   error=0.007256\n",
            "epoch 34/35   error=0.007063\n"
          ]
        }
      ]
    }
  ]
}